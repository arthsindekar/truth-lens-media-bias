

# ğŸŒ **TruthLens â€” Semantic Analysis of Political Bias in News Articles**

### **Authors:**

* **Arth Sindekar**
* **Sheshang Ramesh**

---

## ğŸ“Œ **Overview**

**TruthLens** is an end-to-end machine learning project that automatically predicts political bias â€” **Left**, **Right**, or **Center** â€” from news articles.
Using a combination of **classical NLP models** (Logistic Regression, SVM, Decision Tree) and a **fine-tuned BERT transformer**, the system analyzes linguistic cues, framing patterns, vocabulary, and semantic structure to determine ideological leaning.

This project aims to promote **media transparency**, improve **misinformation analysis**, and develop tools for understanding **ideological framing** in real-world news data.

---

## ğŸ“° **Why Political Bias Detection?**

Media shapes public opinion.
But subtle bias often goes unnoticed:

* Selective framing
* Loaded language
* Emphasis or omission
* Tone and sentiment differences

TruthLens explores whether machine learning can **quantify these patterns**, enabling:

âœ” Bias-aware media consumption
âœ” Fact-checking support tools
âœ” Academic research in media studies
âœ” Automated large-scale bias analysis

---

## ğŸ“‚ **Project Structure**

```
truth-lens-media-bias/
â”‚
â”œâ”€â”€ logistic_regression.py       # TF-IDF + Logistic Regression model
â”œâ”€â”€ svm_model.py                 # TF-IDF + Support Vector Machine
â”œâ”€â”€ Tree_Classifier.py           # Decision Tree Model with feature importance
â”œâ”€â”€ BERT_model.py                # Fine-tuned BERT classifier
â”‚
â”œâ”€â”€ data/                        # Parquet dataset splits
â”‚   â”œâ”€â”€ train-00000-of-00001.parquet
â”‚   â”œâ”€â”€ valid-00000-of-00001.parquet
â”‚   â””â”€â”€ test-00000-of-00001.parquet
â”‚
â””â”€â”€ README.md                    # You are here
```

---

## ğŸ“Š **Dataset**

The project uses two openly available datasets from HuggingFace:

* **Article Bias Prediction Media Splits**
* **BABE (Balanced Annotated Bias Evaluation)**

Total articles: **~34,000**
Labels:

* **0 â€” Left**
* **1 â€” Right**
* **2 â€” Center**

### âš ï¸ Validation Fix

The provided validation split was **heavily imbalanced** (~70% Left), resulting in misleading performance.

â¡ï¸ We created a **10% stratified validation split** to ensure balanced evaluation.

---

## ğŸ§  **Models Implemented**

### âœ” **1. Logistic Regression (TFâ€“IDF)**

* Fast, simple, works well for sparse features
* Achieves **0.77 validation accuracy**, **0.55 test accuracy**
* Pipeline includes 100k TF-IDF vocabulary + bigrams

### âœ” **2. Support Vector Machine**

* Linear SVM with `class_weight="balanced"`
* Achieves **0.786 in-distribution**, **0.548 out-of-distribution**
* Robust baseline for TF-IDF models

### âœ” **3. Decision Tree**

* Provides **feature importance** analysis
* Lower accuracy due to overfitting
* Reveals which political terms influence predictions

### âœ” **4. BERT Transformer**

* Fine-tuned `bert-base-uncased`
* Achieves **0.743 validation**, **0.49 test**
* Performs best semantically but needs more tuning for domain generalization

---





## ğŸ“ˆ **Results Summary**

### **Validation Results (In-Distribution)**

| Model               | Accuracy   |
| ------------------- | ---------- |
| Logistic Regression | **0.7698** |
| SVM                 | **0.7868** |
| Decision Tree       | **0.6728** |
| BERT                | **0.7432** |

### **Test Results (Out-of-Distribution)**

| Model               | Accuracy   |
| ------------------- | ---------- |
| Logistic Regression | **0.5461** |
| SVM                 | **0.5485** |
| Decision Tree       | **0.4500** |
| BERT                | **0.4900** |

---

## ğŸ” **Key Findings**

* Models perform **much better on in-distribution validation** than on unseen test data.
* The main issue: **domain shift** â€” publishers use different vocabulary and linguistic styles.
* TF-IDF models struggle with unseen words.
* BERT captures semantics but needs **more training and deeper fine-tuning**.
* Decision Trees show interpretable political keywords:

  * *â€œdonald johnâ€, â€œmrâ€, â€œfoxâ€, â€œnprâ€, â€œpresident obamaâ€, â€œreutersâ€*, etc.

---

## ğŸ›  **Technologies Used**

* **Python 3.10+**
* **scikit-learn**
* **PyTorch**
* **Hugging Face Transformers**
* **pandas / numpy / tqdm**
* **Matplotlib / Seaborn (optional visualization)**

---

## ğŸ“˜ **Future Improvements**

* Use larger transformer architectures (RoBERTa, DeBERTa, Longformer)
* Add metadata-aware models (publisher, topic, year)
* Apply adversarial domain adaptation
* Perform more aggressive text augmentation
* Build explainability dashboards (Grad-CAM for BERT)

---

## â¤ï¸ **Acknowledgements**

This project was built for:
**CS5100 â€“ Foundations of Artificial Intelligence**

Special thanks to the dataset contributors and the open-source NLP community.



